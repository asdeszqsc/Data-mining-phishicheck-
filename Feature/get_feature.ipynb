{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl, socket\n",
    "from urllib.parse import urlparse\n",
    "import urllib \n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "import socket\n",
    "import whois, sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import tldextract\n",
    "import re\n",
    "from tld import get_tld\n",
    "import requests\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "# 쓰인 classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "# 활용 모듈 및 메소드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature01\n",
    "#긴 URL\n",
    "#0이 피싱, 1이 정상\n",
    "def URL_Length(url):\n",
    "    if len(url) <= 74:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shortening_Service(url):\n",
    "    try :\n",
    "        response = requests.get(url)\n",
    "        for resp in response.history:\n",
    "            if response.status_code == 301 or response.status_code == 302 :\n",
    "                return 0\n",
    "            else :\n",
    "                return 1\n",
    "    except :\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. URL에 @가 유무\n",
    "def having_At_Symbol(url):\n",
    "    try:  \n",
    "        if '@' in url:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature04\n",
    "#// 리다이렉션\n",
    "def double_slash_redirecting(url) :\n",
    "    parse = urlparse(url)\n",
    "    path = parse.path\n",
    "    if '//' in path :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prefix_suffix(url):\n",
    "    if '-' in url :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 서브도메인이 2개 이상인지\n",
    "def having_Sub_Domain(url):\n",
    "    try:\n",
    "        url = remove_www(url)\n",
    "        domain = get_tld(url, as_object=True)\n",
    "        if domain.subdomain == \"\":\n",
    "            return 1\n",
    "        dot = domain.subdomain.count('.')\n",
    "        # dot = 0일때 의심사이트로 분류하는게 더 좋긴 함.\n",
    "        if dot >= 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def remove_www(url):\n",
    "    if 'www.' in url[:12]:\n",
    "        url = url.replace(\"www.\", \"\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Domain_registration_length(url):\n",
    "    try :\n",
    "        total_date = get_total_date(url)\n",
    "        if total_date > 365:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except whois.parser.PywhoisError:\n",
    "        print(\"Exception\")\n",
    "        return 0\n",
    "def get_total_date(url):\n",
    "    try:\n",
    "        domain = whois.whois(url)\n",
    "        if type(domain.expiration_date) is list :\n",
    "            expiration_date = domain.expiration_date[0]\n",
    "        else :\n",
    "            expiration_date = domain.expiration_date\n",
    "        if type(domain.updated_date) is list :\n",
    "            updated_date = domain.updated_date[0]\n",
    "        else :\n",
    "            updated_date = domain.updated_date\n",
    "        \n",
    "        total_date = (expiration_date - updated_date).days\n",
    "    \n",
    "        return total_date\n",
    "    except :\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def favicon(url):\n",
    "    try:\n",
    "        resp = urlopen(url, timeout = 10)\n",
    "        bs = BeautifulSoup(resp)\n",
    "        parse_url = urlparse(url)\n",
    "        tld = get_tld(url, as_object = True)\n",
    "        tag_link = bs.findAll(\"link\", rel=re.compile(\"^shortcut icon$\", re.I))\n",
    "        if fav_ico(url) is 1:\n",
    "            return 1\n",
    "        if not tag_link :\n",
    "            tag_link = bs.findAll(\"link\", rel=re.compile(\"^icon$\", re.I))\n",
    "        if not tag_link :\n",
    "            return 0\n",
    "        for link in tag_link:\n",
    "            fav = link.get('href')\n",
    "            parse_fav = urlparse(fav)\n",
    "\n",
    "            if parse_fav.hostname == None :\n",
    "                return 1\n",
    "            elif tld.domain in fav :\n",
    "                return 1\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def fav_ico(url):\n",
    "    try:\n",
    "        http = httplib2.Http(timeout = 10)\n",
    "        status, response = http.request(url+'/favicon.ico')\n",
    "        if status.status == 200:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature13\n",
    "#<a>태그에 링크\n",
    "def getAnchorResult(url):\n",
    "    try:\n",
    "        tmp = tldextract.extract(url).domain\n",
    "        positiveAnchor = 0\n",
    "        negativeAnchor = 1\n",
    "        soup = BeautifulSoup(urlopen(url, timeout = 10), \"html.parser\", from_encoding = 'iso-8859-1')\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            if 'href' in a_tag.attrs:\n",
    "                tldObj = tldextract.extract(a_tag.attrs['href'])\n",
    "                if ( (tldObj.domain == \"\") | (tldObj.domain == \" \") | (tldObj.domain == None)):\n",
    "                    continue\n",
    "                #print(tldObj.domain)\n",
    "                if ( tldObj.domain == tmp):\n",
    "                    positiveAnchor += 1\n",
    "                else:\n",
    "                    negativeAnchor += 1\n",
    "        ratio = negativeAnchor /(positiveAnchor + negativeAnchor)\n",
    "        if ratio > 0.67:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksInTags(url):\n",
    "    try :\n",
    "        parsed_uri = urlparse(url)\n",
    "        domain = '{uri.netloc}'.format(uri = parsed_uri)\n",
    "        http = httplib2.Http(timeout = 5)\n",
    "        status, response = http.request(url)\n",
    "        metaTags = BeautifulSoup(response, parse_only=SoupStrainer(['meta','script','link']))\n",
    "        matchedDomains =0\n",
    "        unMatchedDomains =0\n",
    "        for tag in metaTags:\n",
    "            content =\"\"\n",
    "            if tag.has_attr('content'):\n",
    "                content += (tag['content'])\n",
    "            if tag.has_attr('src'):\n",
    "                content += (tag['src'])\n",
    "            if tag.has_attr('link'):\n",
    "                content += (tag['link'])\n",
    "            matchObj = re.match(r'([^a-zA-Z\\d]+|http[s]?://)?([a-z0-9|-]+)\\.?([a-z0-9|-]+)\\.([a-z0-9|-]+)',content,re.M|re.I)\n",
    "            if matchObj:\n",
    "                subdomain = matchObj.group(2)\n",
    "                midDomain = matchObj.group(3)\n",
    "                topDomain = matchObj.group(4)\n",
    "                if domain.find(midDomain) != -1:  #we have a url that matches the domain of the site\n",
    "                    matchedDomains +=1\n",
    "                else:\n",
    "                    unMatchedDomains +=1\n",
    "\n",
    "        percentUnmatched = unMatchedDomains/(matchedDomains+unMatchedDomains)\n",
    "\n",
    "        return percentUnmatched\n",
    "    except httplib2.ServerNotFoundError:\n",
    "            print(\"exc getLinksInTags Site is Down\")\n",
    "            retVal = 0\n",
    "            pass\n",
    "    except:\n",
    "            print(\"exc getLinksInTags No tags were returned.  Setting to one\")\n",
    "            retVal = 1\n",
    "            pass\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. form 태그의 action 속성 내에 공백이나 about:blank가 있는지\n",
    "def form_action_Handler(url):\n",
    "    try:\n",
    "        http = httplib2.Http(timeout = 5)\n",
    "        status, response = http.request(url)\n",
    "        tags = BeautifulSoup(response, parse_only=SoupStrainer(['form'], timeout = 5))\n",
    "\n",
    "        if tags.get('action') == \"\" or tags.get('action') == \"about:blank\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature16\n",
    "#Whois에 등록되지 않은 url\n",
    "def abnormalUrl(url):\n",
    "    try:\n",
    "        domain = whois.whois(url)\n",
    "    except :\n",
    "        print(\"Exception\")\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def External_Load_script(url):\n",
    "    try:\n",
    "        http = httplib2.Http(timeout = 5)\n",
    "        status, response = http.request(url)\n",
    "        tags = BeautifulSoup(response, parse_only=SoupStrainer(['script']))\n",
    "        good = 0\n",
    "        bad = 0\n",
    "        if tags.get('src'):\n",
    "            bad += 1\n",
    "        else:\n",
    "            good += 1\n",
    "        if bad / (bad+good) == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. redirection 횟수\n",
    "def count_Redirection(url):\n",
    "    try:\n",
    "        count = 0\n",
    "        res = requests.head(url, allow_redirects=True, timeout = 10)\n",
    "        #request\n",
    "        for resp in res.history:\n",
    "            print(resp)\n",
    "            if resp.status_code == 301 or resp.status_code == 302:\n",
    "                count += 1\n",
    "        if count >= 3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature19\n",
    "#하이퍼링크에 마우스를 가져다 댈 시 상태표시줄 조작\n",
    "def onMouseOver(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        for a_tag in soup.findAll(\"a\"):\n",
    "            #print(a_tag.attrs)\n",
    "            if 'onmouseover' in a_tag.attrs:\n",
    "                #print(a_tag.attrs['onmouseover'])\n",
    "                match = re.search('window.status',a_tag.attrs['onmouseover'])\n",
    "                if match:\n",
    "                    return 0\n",
    "        return 1\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Using_Pop_up_Window\n",
    "def Prompt_in_Popup(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        bs = BeautifulSoup(res.content)\n",
    "        for tag in bs.findAll('script'):\n",
    "            prompt = str(tag)\n",
    "            matching = re.search(r'.*open\\(|prompt\\(.*', prompt)\n",
    "            if matching:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        return 0\n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature22\n",
    "#소스코드내 도메인 이름 존재\n",
    "def DomainName_in_Source(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if tldextract.extract(url).domain in res.text :\n",
    "            return 1\n",
    "        return 0   \n",
    "    except:\n",
    "        print(\"Exception\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "def Length_of_Source(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if len(res.text)> 50000:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. duplicated_Tag\n",
    "def duplicated_Tag(url):\n",
    "    res = requests.get(url)\n",
    "    head = res.text.count(\"</head>\")\n",
    "    body = res.text.count(\"</body>\")\n",
    "    \n",
    "    if head >= 2 or body >= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature25\n",
    "#남은 도메인 수명\n",
    " \n",
    "def Remain_Expiration(url) :\n",
    "    try :\n",
    "        total_date = get_remain_date(url)\n",
    "        if total_date >= 180:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except whois.parser.PywhoisError :\n",
    "        print(\"Exception\")\n",
    "        return 0\n",
    "    \n",
    "def get_remain_date(url) :\n",
    "    try:\n",
    "        domain = whois.whois(url)\n",
    "        if type(domain.expiration_date) is list :\n",
    "            expiration_date = domain.expiration_date[0]\n",
    "        elif domain.expiration_date == None :\n",
    "            return 0\n",
    "        else :\n",
    "            expiration_date = domain.expiration_date\n",
    "        total_date = (expiration_date - datetime.today()).days\n",
    "        return total_date\n",
    "    except :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Disabling_Right_Click(url):\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if \"event.button==2\" in res.text:\n",
    "            return 0\n",
    "        else :\n",
    "            return 1\n",
    "    except :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Web_Traffic 순위(알렉사 데이터베이스 참고)\n",
    "def Alexa_Ranking(url):\n",
    "    try:\n",
    "        http = httplib2.Http()\n",
    "        status, response = http.request(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+url)\n",
    "        rank = BeautifulSoup(response, 'xml').find(\"REACH\")[\"RANK\"]\n",
    "\n",
    "        if not rank:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
